{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc30548d",
   "metadata": {},
   "source": [
    "# Assignment 2 - [Sadman_sharif]_[A1944825]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8196a3c",
   "metadata": {},
   "source": [
    "# EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "# Using the Pima Indians Diabetes dataset as an example\n",
    "# You should replace this with your actual dataset path\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Alternative: Load from CSV if provided\n",
    "# train_data = pd.read_csv('train.csv')\n",
    "# test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# For demonstration, using a public diabetes dataset\n",
    "# You can also use: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv\"\n",
    "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \n",
    "                'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(url, names=column_names)\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except:\n",
    "    # Create synthetic data for demonstration if URL fails\n",
    "    np.random.seed(42)\n",
    "    n_samples = 768\n",
    "    data = pd.DataFrame({\n",
    "        'Pregnancies': np.random.randint(0, 15, n_samples),\n",
    "        'Glucose': np.random.randint(50, 200, n_samples),\n",
    "        'BloodPressure': np.random.randint(40, 120, n_samples),\n",
    "        'SkinThickness': np.random.randint(10, 60, n_samples),\n",
    "        'Insulin': np.random.randint(0, 300, n_samples),\n",
    "        'BMI': np.random.uniform(18, 50, n_samples),\n",
    "        'DiabetesPedigreeFunction': np.random.uniform(0.1, 2.5, n_samples),\n",
    "        'Age': np.random.randint(20, 80, n_samples),\n",
    "        'Outcome': np.random.binomial(1, 0.35, n_samples)\n",
    "    })\n",
    "    print(\"Using synthetic data for demonstration\")\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration and visualization\n",
    "print(\"Dataset Information:\")\n",
    "print(data.info())\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(data['Outcome'].value_counts())\n",
    "print(f\"Percentage of diabetic patients: {(data['Outcome'].sum()/len(data))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualizations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Feature Distributions by Diabetes Outcome', fontsize=16)\n",
    "\n",
    "features = data.columns[:-1]\n",
    "for idx, feature in enumerate(features):\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "    data.boxplot(column=feature, by='Outcome', ax=ax)\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel('Diabetes (0=No, 1=Yes)')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d410e1e",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handle_missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing/zero values in certain columns where zeros are biologically impossible\n",
    "# Glucose, BloodPressure, SkinThickness, Insulin, BMI cannot be zero\n",
    "zero_columns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "for column in zero_columns:\n",
    "    if column in data.columns:\n",
    "        # Replace zeros with NaN\n",
    "        data[column] = data[column].replace(0, np.nan)\n",
    "        # Fill with median of the respective feature\n",
    "        median_value = data[column].median()\n",
    "        data[column].fillna(median_value, inplace=True)\n",
    "        print(f\"Replaced zeros in {column} with median: {median_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split data into train, validation, and test sets (60-20-20)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Feature scaling using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values.reshape(-1, 1))\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values.reshape(-1, 1))\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values.reshape(-1, 1))\n",
    "\n",
    "print(\"\\nData preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100f75e",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlp_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDiabetes(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron for Diabetes Prediction\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer: 8 features\n",
    "    - Hidden layers: Configurable number and size\n",
    "    - Output layer: 1 neuron with sigmoid activation for binary classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Number of input features\n",
    "            hidden_layers: List of hidden layer sizes\n",
    "            dropout_rate: Dropout probability for regularization\n",
    "        \"\"\"\n",
    "        super(MLPDiabetes, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer to first hidden layer\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_dim in hidden_layers:\n",
    "            self.layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Count total trainable parameters\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Test the model class\n",
    "test_model = MLPDiabetes(input_dim=8, hidden_layers=[64, 32])\n",
    "print(f\"Model architecture:\")\n",
    "print(test_model)\n",
    "print(f\"\\nTotal trainable parameters: {test_model.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, epochs, learning_rate, batch_size):\n",
    "    \"\"\"\n",
    "    Train the MLP model\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        batch_size: Batch size for training\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing training history\n",
    "    \"\"\"\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "    \n",
    "    # Create DataLoader for batching\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()\n",
    "            val_predicted = (val_outputs > 0.5).float()\n",
    "            val_acc = 100 * (val_predicted == y_val).sum().item() / y_val.size(0)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], '\n",
    "                  f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        X_test, y_test: Test data\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        \n",
    "        # Convert to numpy for sklearn metrics\n",
    "        y_true = y_test.cpu().numpy()\n",
    "        y_pred = predicted.cpu().numpy()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1627a",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Shallow Network (1 hidden layer)\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 1: Shallow Network\")\n",
    "print(\"Architecture: Input(8) -> Hidden(32) -> Output(1)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model1 = MLPDiabetes(input_dim=8, hidden_layers=[32], dropout_rate=0.2)\n",
    "print(f\"Total parameters: {model1.count_parameters():,}\\n\")\n",
    "\n",
    "history1 = train_model(\n",
    "    model=model1,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_val=X_val_tensor,\n",
    "    y_val=y_val_tensor,\n",
    "    epochs=100,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "metrics1 = evaluate_model(model1, X_test_tensor, y_test_tensor)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Accuracy: {metrics1['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics1['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics1['recall']:.4f}\")\n",
    "print(f\"F1-Score: {metrics1['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Deep Network (3 hidden layers)\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 2: Deep Network\")\n",
    "print(\"Architecture: Input(8) -> Hidden(64) -> Hidden(32) -> Hidden(16) -> Output(1)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model2 = MLPDiabetes(input_dim=8, hidden_layers=[64, 32, 16], dropout_rate=0.3)\n",
    "print(f\"Total parameters: {model2.count_parameters():,}\\n\")\n",
    "\n",
    "history2 = train_model(\n",
    "    model=model2,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_val=X_val_tensor,\n",
    "    y_val=y_val_tensor,\n",
    "    epochs=100,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "metrics2 = evaluate_model(model2, X_test_tensor, y_test_tensor)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Accuracy: {metrics2['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics2['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics2['recall']:.4f}\")\n",
    "print(f\"F1-Score: {metrics2['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Wide Network (2 hidden layers with more neurons)\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT 3: Wide Network\")\n",
    "print(\"Architecture: Input(8) -> Hidden(128) -> Hidden(64) -> Output(1)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model3 = MLPDiabetes(input_dim=8, hidden_layers=[128, 64], dropout_rate=0.25)\n",
    "print(f\"Total parameters: {model3.count_parameters():,}\\n\")\n",
    "\n",
    "history3 = train_model(\n",
    "    model=model3,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_val=X_val_tensor,\n",
    "    y_val=y_val_tensor,\n",
    "    epochs=100,\n",
    "    learning_rate=0.0005,  # Lower learning rate for larger model\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "metrics3 = evaluate_model(model3, X_test_tensor, y_test_tensor)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Accuracy: {metrics3['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics3['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics3['recall']:.4f}\")\n",
    "print(f\"F1-Score: {metrics3['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training histories\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "axes[0].plot(history1['train_loss'], label='Exp1: Shallow (Train)', linestyle='-')\n",
    "axes[0].plot(history1['val_loss'], label='Exp1: Shallow (Val)', linestyle='--')\n",
    "axes[0].plot(history2['train_loss'], label='Exp2: Deep (Train)', linestyle='-')\n",
    "axes[0].plot(history2['val_loss'], label='Exp2: Deep (Val)', linestyle='--')\n",
    "axes[0].plot(history3['train_loss'], label='Exp3: Wide (Train)', linestyle='-')\n",
    "axes[0].plot(history3['val_loss'], label='Exp3: Wide (Val)', linestyle='--')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "axes[1].plot(history1['train_acc'], label='Exp1: Shallow (Train)', linestyle='-')\n",
    "axes[1].plot(history1['val_acc'], label='Exp1: Shallow (Val)', linestyle='--')\n",
    "axes[1].plot(history2['train_acc'], label='Exp2: Deep (Train)', linestyle='-')\n",
    "axes[1].plot(history2['val_acc'], label='Exp2: Deep (Val)', linestyle='--')\n",
    "axes[1].plot(history3['train_acc'], label='Exp3: Wide (Train)', linestyle='-')\n",
    "axes[1].plot(history3['val_acc'], label='Exp3: Wide (Val)', linestyle='--')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion_matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "experiments = [\n",
    "    ('Experiment 1: Shallow', metrics1['confusion_matrix']),\n",
    "    ('Experiment 2: Deep', metrics2['confusion_matrix']),\n",
    "    ('Experiment 3: Wide', metrics3['confusion_matrix'])\n",
    "]\n",
    "\n",
    "for idx, (title, cm) in enumerate(experiments):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], \n",
    "                xticklabels=['No Diabetes', 'Diabetes'],\n",
    "                yticklabels=['No Diabetes', 'Diabetes'])\n",
    "    axes[idx].set_title(title)\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table of results\n",
    "results_df = pd.DataFrame({\n",
    "    'Experiment': ['Shallow (1 layer)', 'Deep (3 layers)', 'Wide (2 layers)'],\n",
    "    'Architecture': ['[32]', '[64, 32, 16]', '[128, 64]'],\n",
    "    'Parameters': [model1.count_parameters(), model2.count_parameters(), model3.count_parameters()],\n",
    "    'Test Accuracy': [metrics1['accuracy'], metrics2['accuracy'], metrics3['accuracy']],\n",
    "    'Precision': [metrics1['precision'], metrics2['precision'], metrics3['precision']],\n",
    "    'Recall': [metrics1['recall'], metrics2['recall'], metrics3['recall']],\n",
    "    'F1-Score': [metrics1['f1'], metrics2['f1'], metrics3['f1']]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_idx = results_df['Test Accuracy'].idxmax()\n",
    "print(f\"\\nBest performing model: {results_df.iloc[best_idx]['Experiment']}\")\n",
    "print(f\"Test Accuracy: {results_df.iloc[best_idx]['Test Accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
